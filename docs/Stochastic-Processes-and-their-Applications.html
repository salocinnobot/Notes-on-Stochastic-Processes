<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Stochastic Processes and their Applications</title>

<script src="site_libs/header-attrs-2.20/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">R Publications from Nicolas Tobon</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Stochastic-Processes-and-their-Applications.html">Stochastic Processes and their Applications</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Stochastic Processes and their
Applications</h1>

</div>


<pre class="r"><code>library(&quot;tseries&quot;)</code></pre>
<div id="stochastic-process" class="section level1">
<h1>Stochastic Process</h1>
<p>In general, the definition of a stochastic process varies much
differently from that of probabilistic concepts that have previously
been studied. The study of stochastic processes can be conceptualized as
the study of probability distributions and probabilities over time.</p>
<p>The project will cover the main conceptual areas surrounding that lay
the foundation for understanding stochastic processes. These are the
following sections:</p>
<ol style="list-style-type: decimal">
<li>Stochastic Processes: Definition and Explanation of Conditions</li>
<li>Properties of Stochastic Process: Increment, Ergodicity,
Stationarity</li>
<li>Common Examples of Stochastic Processes: Random Walk, Brownian
Motion-Wiener Process, Martingale, Markov Chain</li>
<li>Discussion/Reflection</li>
</ol>
<p><em>Commentary is in italics</em></p>
<div id="definition-of-a-stochastic-process" class="section level2">
<h2>Definition of a Stochastic Process</h2>
<p>The definition of a stochastic process can be characterized by the
study of the process that generates random variables on some according
to the following conditions:</p>
<ol style="list-style-type: decimal">
<li>Stochastic Process: A collection of random variables defined on some
common probability space <span class="math inline">\({\Omega, F,
P}\)</span>, indexed by <span class="math inline">\(T\)</span>, and
measured in <span class="math inline">\(S\)</span> which can be modeled
arbitrarily but ensures the following conditions:</li>
</ol>
<p>Let’s take the <em>Bernoulli Process of flipping a fair coin
(probability of heads = 1/2, probability of tails = 1/2)</em> as an
example to demonstrate the intuition for these conditions</p>
</div>
<div id="conditions-of-a-stochastic-process---explained"
class="section level2">
<h2>Conditions of a Stochastic Process - Explained</h2>
<p><strong>Sample space <span
class="math inline">\({\Omega}\)</span>:</strong><br />
1) The set of all outcomes is non-zero. There exists some outcomes from
the generated stochastic process</p>
<p><em>The Bernoulli Process of one flip is the set {H,T}, a Bernoulli
Process of two flips is the set {HH, HT, TH, TT}, and so and so
forth</em></p>
<p><strong><span class="math inline">\({\sigma}\)</span>-algebra <span
class="math inline">\(F\)</span></strong><br />
1) The events which are a set of all subsets of <span
class="math inline">\({\Omega}\)</span>. It can defined as the
collection of all subsets created by the permutations for a specified
outcome length. The conditions for the set is related to the result of
operations performed (complement, union, and intersections) and
inherently allows our events/subsets/etc. to be measured by
probability</p>
<p><em>If our Bernoulli Process of one flip is the set {H, T}, then
{{H}, {T}} is a <span class="math inline">\({\sigma}-algebra\)</span>
but {{H}, {T}, {H}} or {{HT}} is not</em></p>
<p>For reference, a measurable Function: A function between two
measurable spaces that maintains the structure of one space to another
space</p>
<p><strong>Probability measure <span
class="math inline">\({P}\)</span></strong><br />
1) The measure of the entire sample space is one <span
class="math inline">\({P(\Omega) = 1}\)</span></p>
<p><em>In our Bernoulli Process of one flip generates the <span
class="math inline">\(\Omega\)</span> as the set {H, T}, then our
probability measure is <span class="math inline">\(\Sigma \{0.5, 0.5\} =
1\)</span></em></p>
<p><strong>Index Set <span
class="math inline">\({T}\)</span></strong><br />
1) The collection of random variables can be indexed by some set <span
class="math inline">\(T\)</span> (often taken to be discrete or
continuous time) such that the stochastic process can be defined as:
<span class="math inline">\({X(t): t \in T}\)</span></p>
<p><em>In our Bernoulli Process of two flips defined as X(t) t = 1,2,
that generates the <span class="math inline">\(\Omega\)</span> of {HH,
HT, TH, TT} then X(1) = H, X(2) = T is indexed by discrete time</em></p>
<p><strong>State Space <span
class="math inline">\({S}\)</span></strong><br />
1) The values of the random variables must take exist in the defined
mathematical space (i.e. Euclidean)</p>
<p><em>The Bernoulli Process of one flip generates an outcome of either
H or T, but these can be represented by 1 or 0 </em></p>
<p><em>In essence, the importance for defining the random variables on
some common probability space (something that seems inherently
straightforward) is to ensure that one can measure differences in the
process and outcomes, rather than relying on any one probabilistic
distribution governing the process. If one wants to define two different
distributions that govern the process under the same space, the
definition may allows for that</em></p>
</div>
<div id="properties-of-a-stochastic-process" class="section level2">
<h2>Properties of a Stochastic Process</h2>
<p>With the general assumptions of stochastic processes considered,
certain properties can be characterized to model specific types of
stochastic properties.</p>
<p>An alternative view on stochastic processes that might be useful for
the this section,describes the stochastic process as a probability
distribution and is given by the following definition:</p>
<p>The law of the stochastic process seems to be defined as <span
class="math inline">\({\mu = P \circ G}\)</span>, or equivalently the
probability measure across the function defining the “behavior of the
random variable”. The law characterizes the probability distribution of
the stochastic process across time, and for a random walk it is the
probability distribution of the possible trajectories of the walk.</p>
<p>The following characteristics are as such:</p>
<div id="discrete-time-vs-continuous-time-models"
class="section level3">
<h3>Discrete-Time vs Continuous-Time Models</h3>
<p>With the assumption of the any value t from the index set <span
class="math inline">\(T\)</span> equal to the value at time t,
stochastic processes can be further characterized as discrete-time or
continuous-time stochastic models.</p>
<p><strong>Discrete-Time Stochastic Models</strong> Discrete-time
stochastic models are relevant for characterizing points discrete points
in time (i.e. 1, 2, 3, 4)</p>
<p><strong>Continuous-Time Stochastic Models</strong> Continuous-time
stochastic models are relevant for characterizing continuous points in
time (i.e. <span class="math inline">\(t_1 - t_2\)</span>)</p>
</div>
<div id="increment" class="section level3">
<h3>Increment</h3>
<p>The increment can be thought of as the difference between the
stochastic process at two arbitrary times <span
class="math inline">\(t_1, t_2\)</span>. The difference between the two
random variables produced can be though of as an S-valued random
variable, which is equivalent to the path defined by a random variable
between two points.</p>
<p>An increment is often characterized to have properties of being
stationary independent, or both.</p>
<p><strong>Independent Increment</strong> An independent increment acts
as an extension to the idea of independence for any two random
variables, to any the S-valued random variable produced from the
interval indexed by time t. In essence, the probability of the random
path at a time interval t is independent of the random path at any other
time interval.</p>
<p><span class="math inline">\({F_{X_{t_1}}, ..., x_{t_n}(x_1, ..., x_n)
= F_{X_{t_1}}(x_1), ..., F_{X_{t_n}}(x_n)}\)</span> for all <span
class="math inline">\({x_1, ... x_n}\)</span></p>
<p><em>An independent increment ensure that at every point t in the path
function, the path instantaneous path generated is independent.</em></p>
<p>For <span class="math inline">\({t_1, t_2, ..., t_n \in T}\)</span>
with <span class="math inline">\(t_1 &lt; t_2 &lt; ... &lt;
t_n\)</span>, the increments <span class="math inline">\({X_{t_1},
X_{t_2} - X_{t_1}, ..., X_{t_n}-X_{t_{n-1}}}\)</span> are
independent.</p>
<p><em>Such a definition applies mostly to a continuous function as it
ensures that the time increments be the same, resulting in equidistant
intervals. The equidistant, infinitely small intervals are key for
defining any continuous function which is very elegant</em></p>
<p><strong>Stationary Increment</strong> A stationary increment is the
idea that the probability distribution between <span
class="math inline">\({X_t - X_s}\)</span> for two times <span
class="math inline">\({t, s}\)</span>, is dependent only on the scalar
length of the time interval <span
class="math inline">\(s-t\)</span>.</p>
<p>For <span class="math inline">\({t_1, t_2, ..., t_n \in T}\)</span>
with <span class="math inline">\(t_1 &lt; t_2 &lt; ... &lt;
t_n\)</span>, the increments <span class="math inline">\({X_{t_1},
X_{t_2} - X_{t_1}, ..., X_{t_n}-X_{t_{n-1}}}\)</span> are dependent only
on the scalar length of the time interval <span
class="math inline">\(t_j - t_i\)</span></p>
<p><em>For a Wiener process, the probability distribution of <span
class="math inline">\(X_t - X_s\)</span> is normal with mean 0 and
variance t-s. Alternatively, for the Poisson process, the probability
distribution of <span class="math inline">\(X_t-X_s\)</span> is a
Poisson distribution with expected value <span
class="math inline">\(\lambda(t-s)\)</span></em></p>
</div>
<div id="ergodicity" class="section level3">
<h3>Ergodicity</h3>
<p>The ergodicity of a stochastic process is simply the property that
exists if a stochastic process’s ensemble average is equal to it’s time
average at any point <span class="math inline">\(t\)</span>, as the
number of samples drawn <span class="math inline">\(N\)</span> converges
to infinity.</p>
<p><strong>Ensemble Average</strong><br />
The ensemble average is defined as the average value of the several
samples of the random variable at time t.</p>
<p><strong>Time Average</strong> The time average is defined as the
characteristic expected value of the probability distribution.</p>
<p>Naturally, ergodicity expresses the idea that a point of a moving
system, either a dynamical system or a stochastic process, will visit
all parts of the space that the system moves in, in a uniform and random
sense. The definition alternatively states that the average value of a
stochastic process can be preserved either by analyzing the collection
of variables at any point in time t or by analyzing the mean of the
random variable.</p>
<p>When a system is ergodic, it can no longer be reduced.</p>
<p><em>One example of the ergodicity is when observing a call operating
center across a number of operator, where each operator receives a
specific waveform from a caller. If one models the call operating center
across all operators as a stochastic process and probabilistic
waveforms, then the call operating center would be ergodic if the
ensemble averge across all operators across any arbitrary time t is
converges to the squared time average of the expected value of the time
average at the for a growing number of operators.</em></p>
<p><em>One key insight I’ve encountered, is the similarity between the
ergodicity (time average vs ensemble average of stochastic process) and
the concept of estimators (sample average vs population average in
conventional probabilistic settings)</em></p>
<p><em>Another key insight is the relatedness of the idea of
irreducibility for both ergodic systems, and the idea and the
irreducible term of the Least-Squares Solution or any other optimal
solution</em></p>
</div>
<div id="stationarity" class="section level3">
<h3>Stationarity</h3>
<p>The stationarity of a stochastic process is an important assumption
which states that the S-valued random variable produced by the a length
n interval <span class="math inline">\(\tau\)</span> is has the same
distribution as any other length n interval.</p>
<p>The formal definition: <span
class="math inline">\({{F_X}(x_{t_{1+\tau}}, ..., x_{t_{n+\tau}}) =
{F_X}(x_{t_1}, ..., x_{t_n})}\)</span> for all <span
class="math inline">\({{\tau}, t_1, ..., t_n \in R}\)</span> and for all
n <span class="math inline">\({\in N}\)</span></p>
<p>The property of stationarity can be characterized in several ways,
although strict and weak stochastic processes are generally the most
relevant.</p>
<p><strong>Strict Stationarity</strong> A stochastic process is
strict-sense stationary when the distribution does not change at any
point between time t and t+h. Inherently, for any finite sub-sequence of
random variables of the stochastic process across the subsequent time
index, <span class="math display">\[{{F_X}(x_{t_{1+\tau}}, ...,
x_{t_{n+\tau}}) = {F_X}(x_{t_1}, ..., x_{t_n})}\]</span></p>
<p><em>Going back to the Bernoulli process, each draw of our observation
is drawn from the same Bernoulli Distribution and thus the process is
strict-sense stationary</em></p>
<p><strong>Weak Stationarity</strong> Often times, stationarity cannot
be modeled as stringently thus requiring the use of weak stationarity to
characterize stationarity. A stochastic processes is weak stationary
if:</p>
<ol style="list-style-type: decimal">
<li>The mean function is constant for any subsequent difference in time
t</li>
<li>The covariance function depends only on the difference between <span
class="math inline">\({t_1}\)</span> and <span
class="math inline">\({t_2}\)</span> and can be indexed by one variable,
<span class="math inline">\({\tau}\)</span>. Thus, the covariance
assumes the same mean for each random variable, effectively nullifying
the effect but enables the difference in time to define the
covariance.</li>
<li>The second moments must be finite for any time t <span
class="math inline">\({E[|X(t)|^2] &lt; \inf}\)</span> (Remember that
the second moment is the variance of the function)</li>
</ol>
<p>Very simply, one can interpret non-stationary stochastic processes as
those displaying trends or seasonality.</p>
<p><strong>Generating Stationary vs non-stationary data</strong></p>
<pre class="r"><code>len_data = 10000
## Bernoulli Process
stat_data = rbinom(len_data, 1, prob = 0.5)

stat_data[which(stat_data == 0)] = -1
stat_data = cumsum(stat_data)

## Random walk with drift (contains a trend)
n_stat_data = rnorm(len_data, 0, 1)

n_stat_data = rep(0, len_data)
## yt = yt-1 + a + error (norm dist)
for (x in 1:len_data) { 
    if (x != 1) {
      n_stat_data[x] = n_stat_data[x-1] + 0.01 + rnorm(1, 0, 1)
    }
}

n_stat_data = cumsum(n_stat_data)</code></pre>
<p><img src="Stochastic-Processes-and-their-Applications_files/figure-html/unnamed-chunk-3-1.png" width="672" /><img src="Stochastic-Processes-and-their-Applications_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
</div>
<div
id="on-methods-for-transforming-and-analyzing-non-stationary-processes"
class="section level3">
<h3>On methods for transforming and analyzing non-stationary
processes</h3>
<p><em>Differencing</em><br />
Essentially differencing can be seen as differentiating our function to
several magnitudes of order, to obtain a stationary representation of
our stochastic process. Differencing can allow the analyst to model the
data as a stationary process, then undo the differencing to transfer
predicitons to the original data.</p>
<p><em>Autocorrelation function</em><br />
The autocorrelation function is the correlation of a time series with a
lagged version of it’s self. Effectively, the is correlation is
calculated for each range produced by each time series with each
subsequent lagged value of the time series. The autocorrelation function
can be used to define a weak-sense stationary stochastic process</p>
<p><em>Unit root test</em><br />
The unit root test is a hypothesis test for measuring whether a
stochastic process is stationary. By performing the KPSS test according
the null hypothesis that the data are stationary, we can determine
whether the distribution favors.</p>
</div>
</div>
<div id="examples-of-stochastic-processes" class="section level2">
<h2>Examples of Stochastic Processes</h2>
<div id="random-walk-discrete-time" class="section level3">
<h3>Random Walk (Discrete-Time)</h3>
<p>A random walk can be defined as a stochastic process starting at 0
that takes one of two values, either +1 or -1, at any point in time. The
concept of random walks can be extended to more than one dimension, but
is most easily understood as a walk across the one-dimensional number
line.</p>
<pre class="r"><code>gen_walk = function(len_data) {
  walk = rbinom(len_data, 1, prob = 0.5)

  walk[which(walk == 0)] = -1
  walk = cumsum(walk)
  walk

  
}
x_walk = gen_walk(10000)
y_walk = gen_walk(10000)</code></pre>
<p><img src="Stochastic-Processes-and-their-Applications_files/figure-html/unnamed-chunk-5-1.png" width="672" /><img src="Stochastic-Processes-and-their-Applications_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
</div>
<div id="brownian-motion-or-weiner-process-continuous-time"
class="section level3">
<h3>Brownian Motion or Weiner Process (Continuous-Time)</h3>
<p>A Wiener process can be defined as a stochastic process that can
defined as a continuous-time random walk, with the following
assumptions</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\({W_0 = 0}\)</span></li>
<li>W has independent increments</li>
<li>W has Gaussian increments: <span class="math inline">\({W_{t+s} -
W_t}\)</span> is normally distributed with mean 0 and variance s</li>
<li>W has continuous paths: <span class="math inline">\(W_t\)</span> is
continuous in <span class="math inline">\(t\)</span></li>
</ol>
<p><em>Brownian motion is used in computational physics to study of the
random motion of particles suspended in a medium (liquid or
gas)</em></p>
<pre class="r"><code>len_data = 1000
plot(gen_walk(len_data), main = &quot;Wiener Process in One-Dimension&quot;, type = &#39;l&#39;, ylim = c(-500, 500), xlab = &#39;Time&#39;, ylab = &#39;x_walk&#39;)
for (x in 1:10) { 
  lines(gen_walk(len_data), type = &#39;l&#39;, col = sample(rainbow(10)))
}
lines(seq(1, len_data),seq(1, len_data))
lines(seq(1, len_data), -seq(1, len_data))
legend(&quot;topleft&quot;, legend = c(&quot;Variance = t&quot;), lty = 1) </code></pre>
<p><img src="Stochastic-Processes-and-their-Applications_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>plot(x_walk, y_walk, main = &quot;Wiener Process in Two-Dimesions&quot;, type = &#39;l&#39;, xlab = &#39;Time&#39;)</code></pre>
<p><img src="Stochastic-Processes-and-their-Applications_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<p>Interestingly, the expected value of a Wiener process is 0 but the
variance is the Wiener process is t. This means that for further points
in time, the variance of the random variable will be equivalent to <span
class="math inline">\(t\)</span>, but the expected value will not
change.</p>
</div>
<div id="martingale-discretecontinuous-time" class="section level3">
<h3>Martingale (Discrete/Continuous-Time)</h3>
<p>A martingale can be defined as a discrete or continuous time
stochastic process who’s expected value at any point time t, is equal to
the value of the random variable produced before it.</p>
</div>
<div id="markov-processes-discretecontinuous-time"
class="section level3">
<h3>Markov Processes (Discrete/Continuous-Time)</h3>
<p>A markov chain can be defined as a discrete or continuous time
stochastic process who’s probability model is based solely on the
current state. The markov chain in effect has no memory, as the
probability of the future state only depends on the probability from the
current state.</p>
<p>The markov chain is often best modeled with a transition matrix and
state vector, that represents both the probabilities from transferring
from one state to any other state (including itself) and the current
state (with 1 for the column value of the current state, and 0 for all
else).</p>
<p>If the markov chain is stationary, the product of the resulting row
vector and the transition matrix (given that that the resulting row
vector begins with any arbitrary state) should be equal to the resulting
row vector. In essence, the input row vector should converge to the
output row vector.</p>
<pre class="r"><code>ob_len = 100

t_mat = matrix(c(0.3, 0.4, 0.3, 0.2, 0.1, 0.7, 0.5, 0, 0.5), nrow = 3)
row_vec = c(1, 0, 0)
conv_rate = c(0)

for (x in 1:ob_len) {

  
  if (x != 1) {
    new_vec = row_vec%*%t_mat
    conv_rate = append(conv_rate, sum(abs(new_vec-row_vec)))
    row_vec = new_vec
  }

}

plot(conv_rate, main = &quot;Convergence of Steady State Markov Chain&quot;, type = &#39;l&#39;, col = 2)</code></pre>
<p><img src="Stochastic-Processes-and-their-Applications_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><em>The figure shown above models the Convergence of a Steady State
Markov Chain as a function of the sum difference between the current and
previous state</em></p>
<p>Seemingly, the stationary row vector can be described as the
eigenvector of the transition matrix with eigenvalue 1.</p>
</div>
</div>
<div id="reflections" class="section level2">
<h2>Reflections</h2>
<p>The motivation for the project idea, arose by applying previously
explored ideas about clustering to an idea not yet explored. My original
intention was to discover clustering methods in the field of stochastic
processes, a field I just learned about only some days before.
Willingly, I took the opportunity of an open project to ask a question
in a field unbeknownst to be which in hindsight was not the optimal path
for learning. In order for me to attempt to discover this area, I had to
seek understanding for several concepts about stochastic processes, thus
the project is bounded by limited knowledge of stochastic processes.</p>
<p><em>On the questions asked in the presentation today</em> There are
various observations I see at the intersection of the field of
stochastic processes and statistical learning that are noteworthy. For
one, the important ideas governing the “statistics” such as the
independence, expected value on the surface level are defined quite
similarly for both fields and only tend to differ when observing the
inner workings.</p>
<p><strong>Links to further research</strong></p>
<p><a
href="https://www2.econ.iastate.edu/tesfatsi/ArchitectureOfComplexity.HSimon1962.pdf"
class="uri">https://www2.econ.iastate.edu/tesfatsi/ArchitectureOfComplexity.HSimon1962.pdf</a></p>
<p><a href="http://carlmeyer.com/pdfFiles/StochasticComplementation.pdf"
class="uri">http://carlmeyer.com/pdfFiles/StochasticComplementation.pdf</a></p>
<p><a
href="https://www.jmlr.org/papers/volume17/khaleghi16a/khaleghi16a.pdf"
class="uri">https://www.jmlr.org/papers/volume17/khaleghi16a/khaleghi16a.pdf</a></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
